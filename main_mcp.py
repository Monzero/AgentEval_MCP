"""
üöÄ MCP/A2A ENTERPRISE-GRADE AGENTIC GOVERNANCE SYSTEM
====================================================


‚úÖ MCP TOOLS PROVIDED:
Input Agent: validate_topic_definition, check_rubric_completeness, suggest_improvements
Question Agent: generate_initial_question, generate_follow_up_question, analyze_rubric_requirements
Research Agent: research_question, search_documents, extract_information, analyze_document_quality
Output Agent: validate_answer, validate_answer_batch, assess_answer_quality  
Scoring Agent: score_topic, assess_evidence_quality, analyze_scoring_rubric

‚úÖ A2A COMMUNICATION:
- Real-time event broadcasting between agents
- Request-response patterns for tool invocation
- Comprehensive message history and tracing
- Intelligent message routing and delivery

‚úÖ USAGE (SAME AS BEFORE):
python main_mcp.py --test-all           # Test all methods with MCP
python main_mcp.py --test-performance   # Compare MCP vs original
python main_mcp.py --method hybrid      # Test specific method with MCP
python main_mcp.py                      # Default MCP evaluation

Your system now uses modern agent protocols while maintaining 100% backward compatibility!
"""

import os
import sys
import asyncio
import json
import logging
import time
from datetime import datetime
from typing import Dict, List, Optional, Any
from dynamic_mcp_orchestrator import create_dynamic_orchestrator_wrapper


# Add the current directory to path for imports
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)

# Import MCP framework and agents
from mcp_a2a_base import get_agent_manager, Colors

from mcp_orchestrator import (
    MCPAgenticOrchestrator, OptimizedAgenticOrchestrator,  # Both async and sync versions
    test_mcp_orchestrator, test_mcp_orchestrator_sync
)

# Import original components for compatibility
from main import (
    OptimizedConfig, TopicDefinition, print_section, 
    create_sample_topic, save_results, save_summary_csv
)

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def print_mcp_banner():
    """Print MCP/A2A system banner"""
    banner = f"""
{Colors.HEADER}{'='*100}
üöÄ ENTERPRISE AGENTIC CORPORATE GOVERNANCE SYSTEM - MCP/A2A EDITION
{'='*100}{Colors.ENDC}

{Colors.OKGREEN}‚úÖ PROTOCOLS ENABLED:{Colors.ENDC}
   üõ†Ô∏è  MCP (Model Context Protocol) - Standardized tool interfaces
   üì°  A2A (Agent-to-Agent) - Distributed communication  
   üîÑ  Backward Compatibility - Same interfaces, enhanced internals

{Colors.OKCYAN}üèóÔ∏è ARCHITECTURE:{Colors.ENDC}
   ‚Ä¢ 5 Specialized MCP Agents with 15+ tools
   ‚Ä¢ Real-time A2A message bus for coordination
   ‚Ä¢ Pre-computed document processing (same speed optimizations)
   ‚Ä¢ Intelligent fallback and error handling
   ‚Ä¢ Comprehensive monitoring and observability

{Colors.WARNING}‚ö° PERFORMANCE:{Colors.ENDC}
   ‚Ä¢ Same sub-second query performance  
   ‚Ä¢ Added protocol benefits with minimal overhead
   ‚Ä¢ Enhanced scalability and reliability
   ‚Ä¢ Real-time agent health monitoring
"""
    print(banner)

async def test_all_mcp_methods(company: str = "PCJEWELLER"):
    """Test all retrieval methods with MCP architecture"""
    
    retrieval_methods = ["hybrid", "bm25", "vector", "direct"]
    results = {}
    
    print_section("TESTING ALL MCP RETRIEVAL METHODS", 
                 f"Company: {company}\nMethods: {', '.join(retrieval_methods)}", 
                 Colors.HEADER)
    
    for method in retrieval_methods:
        print_section(f"TESTING MCP {method.upper()} METHOD", color=Colors.WARNING)
        
        config = OptimizedConfig(company)
        config.retrieval_method = method
        
        orchestrator = MCPAgenticOrchestrator(config)
        topic = create_sample_topic()
        
        try:
            start_time = time.time()
            result = await orchestrator.evaluate_topic(topic)
            end_time = time.time()
            
            if result["success"]:
                results[method] = {
                    "score": result['scoring']['score'],
                    "confidence": result['scoring']['confidence'],
                    "sources": result['research_summary']['total_sources'],
                    "time": end_time - start_time,
                    "research_time": result['performance_metrics']['research_time'],
                    "iterations": result['research_summary']['iterations'],
                    "mcp_tools": result['mcp_metrics']['total_tools'],
                    "a2a_messages": result['mcp_metrics']['message_statistics']['total_messages']
                }
                
                print(f"   ‚úÖ Score: {result['scoring']['score']}/2")
                print(f"   ‚ö° Total Time: {end_time - start_time:.3f}s")
                print(f"   üîç Research Time: {result['performance_metrics']['research_time']:.3f}s")
                print(f"   üìö Sources: {result['research_summary']['total_sources']}")
                print(f"   üõ†Ô∏è MCP Tools Used: {result['mcp_metrics']['total_tools']}")
                print(f"   üì° A2A Messages: {result['mcp_metrics']['message_statistics']['total_messages']}")
                
                save_results(result, config)
            else:
                print(f"   ‚ùå Failed: {result.get('error', 'Unknown error')}")
        
        except Exception as e:
            logger.error(f"Error testing {method}: {e}")
            print(f"   ‚ùå Exception: {e}")
        
        finally:
            await orchestrator.shutdown()
    
    # Print comparison table
    print_section("MCP RETRIEVAL METHOD COMPARISON", color=Colors.HEADER)
    
    if results:
        print(f"{'METHOD':<10} {'SCORE':<8} {'TOTAL':<10} {'RESEARCH':<10} {'SOURCES':<8} {'TOOLS':<8} {'MSG':<6} {'CONF'}")
        print("-" * 85)
        
        for method, metrics in results.items():
            print(f"{method.upper():<10} {metrics['score']}/2{'':<5} {metrics['time']:.2f}s{'':<5} {metrics['research_time']:.2f}s{'':<5} {metrics['sources']:<8} {metrics['mcp_tools']:<8} {metrics['a2a_messages']:<6} {metrics.get('confidence', 'N/A')}")
    
    return results

async def test_mcp_performance_comparison(company: str = "PCJEWELLER"):
    """Compare MCP vs original performance (simulation)"""
    
    print_section("MCP vs ORIGINAL PERFORMANCE COMPARISON", 
                 f"Company: {company}", Colors.HEADER)
    
    topic = create_sample_topic()
    
    # Test MCP version
    print_section("TESTING MCP VERSION", color=Colors.OKGREEN)
    config_mcp = OptimizedConfig(company)
    config_mcp.retrieval_method = "hybrid"
    
    orchestrator_mcp = MCPAgenticOrchestrator(config_mcp)
    
    start_time = time.time()
    result_mcp = await orchestrator_mcp.evaluate_topic(topic)
    mcp_time = time.time() - start_time
    
    await orchestrator_mcp.shutdown()
    
    if result_mcp["success"]:
        mcp_metrics = {
            "total_time": mcp_time,
            "research_time": result_mcp['performance_metrics']['research_time'],
            "score": result_mcp['scoring']['score'],
            "sources": result_mcp['research_summary']['total_sources'],
            "iterations": result_mcp['research_summary']['iterations'],
            "tools_used": result_mcp['mcp_metrics']['total_tools'],
            "messages_sent": result_mcp['mcp_metrics']['message_statistics']['total_messages'],
            "protocol_overhead": result_mcp['performance_metrics'].get('mcp_overhead', 0.0)
        }
        
        print(f"   ‚ö° MCP Total Time: {mcp_metrics['total_time']:.3f}s")
        print(f"   üîç Research Time: {mcp_metrics['research_time']:.3f}s")  
        print(f"   üìä Score: {mcp_metrics['score']}/2")
        print(f"   üõ†Ô∏è Tools Used: {mcp_metrics['tools_used']}")
        print(f"   üì° Messages: {mcp_metrics['messages_sent']}")
        print(f"   ‚è±Ô∏è Protocol Overhead: {mcp_metrics['protocol_overhead']:.3f}s")
        
        save_results(result_mcp, config_mcp)
    
    # Print comparison summary
    print_section("PERFORMANCE COMPARISON SUMMARY", color=Colors.HEADER)
    
    if result_mcp["success"]:
        print(f"üöÄ MCP/A2A VERSION:")
        print(f"   Total Time: {mcp_metrics['total_time']:.3f}s")
        print(f"   Research Time: {mcp_metrics['research_time']:.3f}s")
        print(f"   Protocol Benefits: Enhanced observability, distributed architecture")
        print(f"   Score: {mcp_metrics['score']}/2")
        print(f"   Tools: {mcp_metrics['tools_used']} MCP tools available")
        print(f"   Messages: {mcp_metrics['messages_sent']} A2A messages")
        
        print(f"\nüåü KEY MCP/A2A ADVANTAGES:")
        print(f"   ‚Ä¢ Standardized tool interfaces for interoperability")
        print(f"   ‚Ä¢ Real-time agent communication and monitoring")
        print(f"   ‚Ä¢ Distributed architecture for horizontal scaling")
        print(f"   ‚Ä¢ Enhanced debugging with message tracing")
        print(f"   ‚Ä¢ Future-proof protocol compliance")
        print(f"   ‚Ä¢ Same performance with added enterprise features")

def test_single_mcp_method(company: str = "PCJEWELLER", method: str = "hybrid"):
    """Test single method with MCP (synchronous wrapper)"""
    
    print_section("MCP SINGLE METHOD TEST", 
                 f"Company: {company}, Method: {method}", Colors.HEADER)
    
    config = OptimizedConfig(company)
    config.retrieval_method = method
    
    # Use synchronous wrapper for compatibility
    ## orchestrator = OptimizedAgenticOrchestrator(config)
    if getattr(config, 'enable_dynamic_orchestration', True):
        print(f"{Colors.OKGREEN}üöÄ Using Dynamic AI-Driven Orchestrator{Colors.ENDC}")
        orchestrator = create_dynamic_orchestrator_wrapper(config)
    else:
        print(f"{Colors.OKCYAN}üèõÔ∏è Using Static MCP Orchestrator{Colors.ENDC}")
        orchestrator = OptimizedAgenticOrchestrator(config)  # Sync wrapper

    topic = create_sample_topic()
    
    try:
        result = orchestrator.evaluate_topic(topic)
        
        if result["success"]:
            print_section("MCP TEST COMPLETED", 
                         f"Score: {result['scoring']['score']}/2\n" +
                         f"Total Time: {result['performance_metrics']['total_time']:.3f}s\n" +
                         f"Research Time: {result['performance_metrics']['research_time']:.3f}s\n" +
                         f"Sources: {result['research_summary']['total_sources']}\n" +
                         f"Method: MCP {method}\n" +
                         f"Tools: {result['mcp_metrics']['total_tools']}\n" +
                         f"Messages: {result['mcp_metrics']['message_statistics']['total_messages']}",
                         Colors.OKGREEN)
            save_results(result, config)
            return result
        else:
            print_section("MCP TEST FAILED", result.get('error'), Colors.FAIL)
            return result
    
    except Exception as e:
        print_section("MCP TEST ERROR", str(e), Colors.FAIL)
        return {"success": False, "error": str(e)}
    
    finally:
        orchestrator.shutdown()

def run_default_mcp_evaluation(company: str = "PCJEWELLER"):
    """Run default MCP evaluation (synchronous)"""
    
    print_section("DEFAULT MCP EVALUATION", color=Colors.HEADER)
    
    config = OptimizedConfig(company)
    #orchestrator = OptimizedAgenticOrchestrator(config)  # Sync wrapper
    if getattr(config, 'enable_dynamic_orchestration', True):
        print(f"{Colors.OKGREEN}üöÄ Using Dynamic AI-Driven Orchestrator{Colors.ENDC}")
        orchestrator = create_dynamic_orchestrator_wrapper(config)
    else:
        print(f"{Colors.OKCYAN}üèõÔ∏è Using Static MCP Orchestrator{Colors.ENDC}")
        orchestrator = OptimizedAgenticOrchestrator(config)  # Sync wrapper
    topic = create_sample_topic()
    
    try:
        result = orchestrator.evaluate_topic(topic)
        
        if result["success"]:
            print_section("MCP EVALUATION COMPLETED", 
                         f"Final Score: {result['scoring']['score']}/2\n" +
                         f"Evidence Quality: {result['scoring']['evidence_quality']}\n" +
                         f"Total Time: {result['performance_metrics']['total_time']:.3f}s\n" +
                         f"Research Time: {result['performance_metrics']['research_time']:.3f}s\n" +
                         f"Sources: {result['research_summary']['total_sources']}\n" +
                         f"Method: MCP {result['research_summary']['retrieval_method']}\n" +
                         f"Protocol: MCP/A2A Enabled",
                         Colors.OKGREEN)
            
            save_results(result, config)
            return result
        else:
            print_section("EVALUATION FAILED", result.get('error'), Colors.FAIL)
            return result
    
    except Exception as e:
        print_section("EVALUATION ERROR", str(e), Colors.FAIL)
        return {"success": False, "error": str(e)}
    
    finally:
        orchestrator.shutdown()

async def demonstrate_mcp_capabilities():
    """Demonstrate MCP-specific capabilities"""
    
    print_section("MCP CAPABILITIES DEMONSTRATION", color=Colors.HEADER)
    
    config = OptimizedConfig("PCJEWELLER")
    orchestrator = MCPAgenticOrchestrator(config)
    
    try:
        # 1. Show agent discovery
        print_section("1. AGENT DISCOVERY")
        capabilities = orchestrator.agent_manager.get_agent_capabilities()
        
        print(f"   üîç Discovered {len(capabilities)} agents:")
        for agent_id, tools in capabilities.items():
            print(f"     ‚Ä¢ {agent_id}: {len(tools)} tools available")
            for tool in tools[:2]:  # Show first 2 tools
                print(f"       - {tool['name']}: {tool['description'][:60]}...")
        
        # 2. Show system status
        print_section("2. SYSTEM STATUS")
        status = await orchestrator.get_system_status()
        
        print(f"   üìä System Status: {status['orchestrator_status']}")
        print(f"   ü§ñ Active Agents: {status['agent_count']}")
        print(f"   üõ†Ô∏è Total Tools: {status['total_tools']}")
        print(f"   üì° Message Bus: {status['message_bus_status']}")
        
        # 3. Demonstrate tool invocation
        print_section("3. DIRECT TOOL INVOCATION")
        
        # Call a specific tool directly
        doc_metadata = await orchestrator.research_agent.call_tool("get_document_metadata", {
            "include_statistics": True
        })
        
        print(f"   üìã Available Documents: {doc_metadata['total_documents']}")
        if doc_metadata['total_documents'] > 0:
            print(f"   üìä System Statistics:")
            if 'system_statistics' in doc_metadata:
                stats = doc_metadata['system_statistics']
                print(f"     ‚Ä¢ Total Pages: {stats.get('total_pages', 0)}")
                print(f"     ‚Ä¢ Total Chunks: {stats.get('total_chunks', 0)}")
                print(f"     ‚Ä¢ Cache Status: {stats.get('cache_status', 'unknown')}")
        
        # 4. Show message tracing
        print_section("4. A2A MESSAGE TRACING")
        
        # Perform a simple validation to generate messages
        sample_topic = create_sample_topic()
        validation_result = await orchestrator.input_guardrail_agent.call_tool("validate_topic_definition", {
            "topic": {
                "topic_name": sample_topic.topic_name,
                "goal": sample_topic.goal,
                "guidance": sample_topic.guidance,
                "scoring_rubric": sample_topic.scoring_rubric
            }
        })
        
        print(f"   üì° Message History: {len(orchestrator.message_bus.message_history)} messages")
        print(f"   ‚úÖ Validation Result: {validation_result.get('valid', False)}")
        
        # Show recent messages
        recent_messages = orchestrator.message_bus.message_history[-3:]
        for i, msg in enumerate(recent_messages, 1):
            print(f"     {i}. {msg.from_agent} ‚Üí {msg.to_agent}: {msg.message_type}")
        
        print_section("MCP CAPABILITIES DEMONSTRATED", 
                     "Ready for production use with enhanced observability", Colors.OKGREEN)
    
    except Exception as e:
        print_section("DEMONSTRATION ERROR", str(e), Colors.FAIL)
    
    finally:
        await orchestrator.shutdown()

def show_usage_instructions():
    """Show usage instructions for the MCP system"""
    
    usage = f"""
{Colors.HEADER}MCP/A2A USAGE INSTRUCTIONS{Colors.ENDC}

{Colors.OKGREEN}Basic Usage:{Colors.ENDC}
  python main_mcp.py                      # Default MCP evaluation
  python main_mcp.py --method hybrid      # Test specific method with MCP
  python main_mcp.py --method bm25        # Test BM25 with MCP protocols
  python main_mcp.py --method vector      # Test vector search with MCP
  python main_mcp.py --method direct      # Test direct method with MCP

{Colors.OKCYAN}Advanced Testing:{Colors.ENDC}
  python main_mcp.py --test-all           # Test all methods with MCP
  python main_mcp.py --test-performance   # MCP performance analysis
  python main_mcp.py --demo-capabilities  # Demonstrate MCP features
  python main_mcp.py --async              # Run in async mode

{Colors.WARNING}Integration:{Colors.ENDC}
  # Use in your code (backward compatible)
  from main_mcp import OptimizedAgenticOrchestrator
  config = OptimizedConfig("YOUR_COMPANY")
  orchestrator = OptimizedAgenticOrchestrator(config)
  result = orchestrator.evaluate_topic(topic)

{Colors.HEADER}Key Differences from Original:{Colors.ENDC}
  ‚úÖ Same interfaces and functionality
  ‚úÖ Enhanced with MCP tools and A2A communication
  ‚úÖ Better observability and monitoring
  ‚úÖ Distributed architecture ready
  ‚úÖ Protocol-compliant for future integrations
"""
    print(usage)

def main():
    """Main function for MCP-enabled agentic system"""
    
    print_mcp_banner()
    
    if len(sys.argv) > 1:
        arg = sys.argv[1]
        
        if arg == "--test-all":
            asyncio.run(test_all_mcp_methods())
        
        elif arg == "--test-performance":
            asyncio.run(test_mcp_performance_comparison())
        
        elif arg == "--demo-capabilities":
            asyncio.run(demonstrate_mcp_capabilities())
        
        elif arg == "--async":
            # Run async test
            asyncio.run(test_mcp_orchestrator())
        
        elif arg == "--method" and len(sys.argv) > 2:
            method = sys.argv[2]
            test_single_mcp_method(method=method)
        
        elif arg == "--help" or arg == "-h":
            show_usage_instructions()
        
        else:
            print(f"Unknown argument: {arg}")
            show_usage_instructions()
    
    else:
        # Default: run MCP evaluation
        run_default_mcp_evaluation()

# Export key classes and functions for backward compatibility
__all__ = [
    'OptimizedAgenticOrchestrator',  # Sync wrapper (backward compatible)
    'MCPAgenticOrchestrator',        # Async MCP orchestrator  
    'OptimizedConfig',               # Configuration (same as original)
    'TopicDefinition',               # Topic definition (same as original)
    'create_sample_topic',           # Sample topic creator
    'save_results',                  # Results saving (same as original)
    'save_summary_csv'               # CSV export (same as original)
]

# Integration examples
def integration_example_sync():
    """Example of synchronous integration (backward compatible)"""
    
    # This code works exactly like the original system
    config = OptimizedConfig("PCJEWELLER")
    config.retrieval_method = "hybrid"
    
    #orchestrator = OptimizedAgenticOrchestrator(config)
    if getattr(config, 'enable_dynamic_orchestration', True):
        print(f"{Colors.OKGREEN}üöÄ Using Dynamic AI-Driven Orchestrator{Colors.ENDC}")
        orchestrator = create_dynamic_orchestrator_wrapper(config)
    else:
        print(f"{Colors.OKCYAN}üèõÔ∏è Using Static MCP Orchestrator{Colors.ENDC}")
        orchestrator = OptimizedAgenticOrchestrator(config)  # Sync wrapper
    topic = create_sample_topic()
    
    try:
        result = orchestrator.evaluate_topic(topic)
        
        if result["success"]:
            print(f"Score: {result['scoring']['score']}/2")
            print(f"Enhanced with MCP: {result['research_summary'].get('mcp_enabled', False)}")
            save_results(result, config)
        
        return result
    
    finally:
        orchestrator.shutdown()

async def integration_example_async():
    """Example of async integration with full MCP capabilities"""
    
    config = OptimizedConfig("PCJEWELLER")
    orchestrator = MCPAgenticOrchestrator(config)
    topic = create_sample_topic()
    
    try:
        # Access individual agents directly
        validation = await orchestrator.input_guardrail_agent.call_tool("validate_topic_definition", {
            "topic": {
                "topic_name": topic.topic_name,
                "goal": topic.goal,
                "guidance": topic.guidance,
                "scoring_rubric": topic.scoring_rubric
            }
        })
        
        if validation["valid"]:
            # Full evaluation
            result = await orchestrator.evaluate_topic(topic)
            
            if result["success"]:
                print(f"MCP Score: {result['scoring']['score']}/2")
                print(f"Tools Used: {result['mcp_metrics']['total_tools']}")
                print(f"Messages: {result['mcp_metrics']['message_statistics']['total_messages']}")
                save_results(result, config)
            
            return result
        
    finally:
        await orchestrator.shutdown()

if __name__ == "__main__":
    main()
